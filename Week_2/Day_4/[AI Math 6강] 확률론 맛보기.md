# [AI Math 6강] 확률론 맛보기

딥러닝의 기본 바탕이되는 확률론에 대해 소개합니다. **확률분포, 조건부확률, 기대값**의 개념과 **몬테카를로 샘플링** 방법을 설명합니다. 데이터의 초상화로써 확률분포가 가지는 의미와 이에 따라 분류될 수 있는 이산확률변수, 연속확률변수의 차이점에 대해 설명합니다.

 

확률변수, 조건부확률, 기대값 등은 확률론의 매우 기초적인 내용이며 이를 정확히 이해하셔야 바로 다음 강의에서 배우실 통계학으로 이어질 수 있습니다. 기대값을 계산하는 방법, 특히 확률분포를 모를 때 몬테카를로 방법을 통해 기댓값을 계산하는 방법 등은 머신러닝에서 매우 빈번하게 사용되므로 충분히 공부하시고 넘어가시기 바랍니다.



# 1. 확률론의 필요성

- 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있습니다 

- 기계학습에서 사용되는 손실함수(loss function)들의 작동원리는 데이터 공간을 통계적으로 해석해서 유도하게됩니다

  - 예측이 틀렸을 때의 위험(risk)을 최소화하도록 데이터를 학습하는 원리는 통계적 기계학습의 기본 원리이다.

- 회귀분석에서 손실함수로 사용되는 $L_2$-노름은 예측 오차의 분산을 가장 최소화하는 방향으로 학습 하도록 유도 한다.

- 분류 문제에서 사용되는 Cross-entropy는 모델 에측의 불확실성을 최소화하는 방향으로 학습하도록 유도한다.

- 분산 및 불확실성을 최소화 하기 위해서는 측정하는 방법을 알아야한다.

  - 두 대상 혹은 집단을 측정하는 방법을 통계학에서 제공한다.

  

# 2. 확률분포는 데이터의 초상화

- 데이터 공간을 $\mathscr{X}\times\mathscr{Y}$라고 표기하고 $\mathscr{D}$는 데이터 공간에서의 데이터를 추출하는 분포이다.

- 이 때, 데이터는 확률변수 $(\mathbf{x}, y) \sim \mathscr{D}$라고 표기한다.

  - 여기서 $(\mathbf{x}, y)\in\mathscr{X}\times\mathscr{Y}$는 데이터 공간 상의 관측가능한 데이터에 해당한다.

  ![image-20210129012427273](https://user-images.githubusercontent.com/38639633/106174784-c02df680-61d8-11eb-977f-1ba21eb9ea81.png)

  

## 2.1. 이산확률변수 vs 연속확률변수

- 확률변수는 확률분포 $\mathscr{D}$에 따라 이산형(discrete)과 연속형(continuous)확률변수로 구분하게 된다. 

- **이산형 확률변수**는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해 모델링한다.

  
  $$
  \mathbb{P}(X\in A) = \sum_{\mathbf{x}\in A}P(X=\mathbf{x})
  $$
  

- **연속형 확률변수**는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서 적분을 통해 모델링한다.

  
  $$
  \mathbb{P}(X\in A) = \int_AP(\mathbf{x})d\mathbf{x}
  $$

  > 밀도는 누적확률분포의 변화율을 모델링하며 확률로 해석하면 안된다.

- 결합분포 $P(\mathbf{x},y)$는 $\mathscr{D}$를 모델링한다. 

  - 여기서 $\mathscr{D}$는 이론적으로 미리 존재하는 확률분포라고 가정하기 때문에 사전에 알 수 없다. 

- $P(\mathbf{x})$는 입력 $\mathbf{x}$에 대한 주변확률분포(marginal probablility distribution)로 $y$에 대한 정보를 주진 않는다. 

  - 단지, 각 x에 대한 y값의 빈도를 계산하여 분포로 나타낸다.

    ![image-20210129014543679](https://user-images.githubusercontent.com/38639633/106174834-c7550480-61d8-11eb-8d81-652be7fbb1d9.png){:width = "45"}{: .center}![image-20210129014631460](https://user-images.githubusercontent.com/38639633/106174839-c7ed9b00-61d8-11eb-9ecc-764596663c50.png){:width = "45"}{: .center}

- 조건부확률분포 $P(\mathbf{x}|y)$는 데이터 공간에서 입력 $\mathbf{x}$와 $y$의 관계를 모델링한다. 

  - 특정 클래스 $y$가 주어졌다고 가정했을 때, 데이터의 확률분포를 보여준다. 



<br>



# 3. 조건부확률과 Machine Learning

- 조건부확률 $P(y~|~\mathbf{x})$는 입력변수 $\mathbf{x}$에 대한 정답이 $y$일 확률을 의미한다. 

  - 연속확률분포의 경우 ~~확률이 아니고~~ 밀도로 해석함을 주의하자.

- 로지스틱회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용된다.

- 분류문제에서 $softmax(\mathbf{W}\phi+\mathbf{b})$는 데이터 $\mathbf{x}$로부터 추출된 특징패턴 $\phi(\mathbf{x})$과 가중치행렬 $\mathbf{W}$을 통해 조건부확률 $P(y~|~\mathbf{x})$을 계산한다...

  - $P(y~|~\phi(\mathbf{x}))$라고 써도 같은 맥락이다. 

- 회귀문제의 경우 조건부기대값 $\mathbb{E}[y~|~\mathbf{x}]$을 추정한다.

  - 여기서 $\mathbb{E}[y~|~\mathbf{x}] = \int_yyP(y~|~\mathbf{x})dy$이다. 
  - 조건부기대값 $\mathbb{E}[y~|~\mathbf{x}]$은 $\mathbb{E}||y-f(\mathbf{x})||_2$를 최소화하는 함수 $f(x)$와 일치한다.

  <br>

## 3.1. 기대값이란?

- 확률 분포가 주어지면 데이터를 분석하는데 사용 가능한 여러 종류의 통계적 범함수(statistical functional)를 계산할 수 있다.

- 기대값(expectation)은 데이터를 대표하는 통계량이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산 하는데 사용된다.

  > **연속확률분포의 경우 적분**
  > $$
  > \mathbb{E}_{\mathbf{x}\sim P(\mathbf{x})}[f(\mathbf{x})] = \int_{\mathcal{X}}f(\mathbf{x})P(\mathbf{x})d\mathbf{x}
  > $$
  > **이산확률분포의 경우 급수**
  > $$
  > \mathbb{E}_{\mathbf{x}\sim P(\mathbf{x})}[f(\mathbf{x})] = \sum_{\mathbf{x}\in\mathcal{X}}f(\mathbf{x})P(\mathbf{x})
  > $$

- 기대값을 통해 분산, 첨도, 왜도, 공분산 등 다양한 통계량을 계산할 수 있다. 

- 딥러닝은 다층신경망(MLP)을 사용하여 데이터로부터 특징패턴 $\phi$을 추출한다.

<br>

# 4. 몬테카를로 샘플링

- 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분이다

- 확률분포를 모를 때 데이터를 이용하여 기대값을 계산 하려면 **몬테카를로(MonteCarlo)** 샘플링 방법을 사용해야한다.

  - 몬테카를로는 이산형, 연속형에 상관없이 성립한다.

- 몬테카를로 샘플링은 **독립추출**만 보장된다면, 대수의 법칙에 의해 수렴을 보장한다.
  $$
  \mathbb{E}_{\mathbf{x}\sim P(\mathbf{x})}[f(\mathbf{x})]\approx \frac{1}{N}\sum^N_{i=1}f(\mathbf{x}^{(i)}),~~ \mathbf{x}^{(i)} \stackrel {i.i.d}{\sim} P(\mathbf{x})
  $$
  









































































