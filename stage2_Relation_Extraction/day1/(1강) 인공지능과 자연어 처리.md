# (1강) 인공지능과 자연어 처리

**강의 소개**

 이제 본격적으로 자연어처리 공부를 시작해보겠습니다! 🥰 

인공지능이 발전함에 따라 자연어와 관련된 여러 Task들을 해결할 수 있게 되었습니다.

이번 강의에는 인공지능의 발전이 어떻게 자연어 처리 분야에 영향을 주고 발전하게 되었는지 알아봅니다.

또한 앞으로의 과정을 위해 배경지식이 될 수 있는 자연어처리의 기초에 대해서 소개합니다.

그리고 ‘언어모델’의 개념이 어떻게 탄생하게 됐는지 살펴보겠습니다.



## 인공지능의 탄생과 자연어처리

### 자연어 처리 소개

- 피그말리온과 갈리테이아

	- 피그말리온이 여성의 결점을 제거해서 만든 조각상
	- 인간을 대체하는 **인공물**에 대한 최초의 기록
		- 인공지능 : 인간의 지능이 가지는 학습, 추리, 적응, 논증 따위의 기능을 갖춘 컴퓨터 시스템을 말한다.

- 콜로서스 컴퓨터 

	- 프로그래밍이 가능한 세계 최초의 전자식 컴퓨터
	- 1500개의 진공관을 이용해 Boolean 논리 연산을 수행
	- 이 컴퓨터를 만든 사람중 하나인 **앨런 튜링**은 이미테이션 게임이라는 개념을 만듬

- 이미테이션 게임(튜링 테스트)

	- 기계에 지능이 있는지를 판별하고자 하는 실험

	- 인간의 정의나 인간의 지능을 정의하기 어렵다

	- 하지만, 인간이 보기에 **인간 같은 것**을 인간에 준하는 지능이 있다고 간주

	- 컴퓨터가 인간처럼 대화를 할 수 있다면 그 컴퓨터는 인간처럼 사고할 수 있다.

		![image](https://user-images.githubusercontent.com/38639633/114556123-5be0e600-9ca3-11eb-8571-53b7314822a6.png)

	- 즉, 자연어처리를 근간으로 시작했다고 볼 수 있다.

- AI의 황금기
	- general purpose ai를 만들기 위해 자연어 연구가 폭발적 관심을 받게 됨
	- 대표적인 **ELIZA** 챗봇
	- 심리상담사의 역할을 하도록 설계
	- 나 지금 너무 우울해 $\rightarrow$ 왜 우울하세요?
	- 최초의 대화형 채솟
	- 튜링테스트를 적용할 수 있는 최초의 **Human-like AI**



### 자연어처리의 응용분야

- 멀티모달과 자연어가 합쳐진 서비스가 더 두각을 나타낼 것이다. 

- 인간의 자연어처리

	![image](https://user-images.githubusercontent.com/38639633/114645198-e4976a80-9d13-11eb-81c3-d17c408bd94f.png)

	- 대화의 단계는 다음과 같다.
		1. 화자는 자연어 형태로 객체를 인코딩
		2. 메시지의 전송
		3. 청자는 자연어를 객체로 디코딩(본인의 경험적 지식적 내용을 바탕으로 디코딩)

- 컴퓨터의 자연어처리

	![image](https://user-images.githubusercontent.com/38639633/114645264-14df0900-9d14-11eb-8cd6-56600aa17348.png)

	- 대화의 단계
		1. Encoder는 벡터 형태로 자연어를 인코딩
		2. 메시지의 전송
		3. Decoder는 벡터를 자연어로 디코딩



**자연어를 컴퓨터가 이해할 수 있게 수학적으로 어떻게 이쁘게 인코딩할 수 있는지를 살펴보는 것이 이번 강의의 목표!**

- 다영한 자연어처리 application

	![image](https://user-images.githubusercontent.com/38639633/114645362-48ba2e80-9d14-11eb-815f-2efe33a70c23.png)

	- 대부분의 자연어처리 문제는 '분류'의 문제라고 할 수있다.



### 자연어 단어 임베딩

**특징 추출과 분류**

- 분류를 위해서는 데이터를 수학적으로 표현
- 먼저 분류 대상의 특징을 파악해야한다.(Feature extraction)
- 분류 대상의 특징을 기준으로 분류 대상을 그래프 위에 표현 가능하다
- 분류 대상들의 경계를 수학적으로 나눌 수 있다(Classification)
- 새로운 데이터 역시 특징을 기준으로 그래프에 표현하면, 어떤 그룹과 유사한지 파악이 가능하다.

![image](https://user-images.githubusercontent.com/38639633/114645475-87e87f80-9d14-11eb-87ce-c4dd3659e5bb.png)

- 과거에는 사람이 특징을 파악해서 분류했다.
- 실제 복잡한 문제들에서 분류 대상의 특징을 사람이 파악하기 어려울 수 있음
- 이러한 특징을 컴퓨터가 스스로 찾고, 스스로 분류하는 것이 **기계학습**의 핵심이다.

![image](https://user-images.githubusercontent.com/38639633/114645552-ab132f00-9d14-11eb-8bd5-1a5d63304a56.png)



**Word2Vec**

- 자연어를 어떻게 좌표형면 위에 표현할 수 있을까?

- 가장 단순한 방법은 원-핫-인코딩 $\rightarrow$ Sparse representation

	- 단어의 갯수만큼 차원이 늘어난다.
	- 10만단어 >> 10만차원의 벡터공간 필요

	![image](https://user-images.githubusercontent.com/38639633/114645762-05ac8b00-9d15-11eb-963f-b21a529063c7.png)

- 이같은 `단점`을 보완하기 위해 만들어진 것이 `Word2Vec`

![image](https://user-images.githubusercontent.com/38639633/114645846-24128680-9d15-11eb-94dd-4c97f6116480.png)

- 하지만 이를 포함한 문장을 제시해준다면 의미는 이해하기 쉽다.

	![image-20210414113340743](C:\Users\doyeon\AppData\Roaming\Typora\typora-user-images\image-20210414113340743.png)

- 이와 같은 원리로 word2vec은 학습하게 된다. 

	![image](https://user-images.githubusercontent.com/38639633/114645969-5a500600-9d15-11eb-886e-4628bf61f9d0.png)

- Word2Vec은 임베딩 공간에 표현된 단어들의 벡터 연산이 가능하다. 

	![image](https://user-images.githubusercontent.com/38639633/114647770-a18bc600-9d18-11eb-8cad-a62dad20e71b.png)

- 장점도 있지만, 구조적으로 비슷한 의미(여러 도시들)의 경우, 주변 subword가 비슷한 점에서 비슷한 단어로 임베딩한다는 단점이 존재한다.



**Fasttext**

- 위와 같은 단점을 보완하기 위해 만든 임베딩 방식이 **Fasttext**이다.

	![image](https://user-images.githubusercontent.com/38639633/114648109-37bfec00-9d19-11eb-89a8-976de5b12970.png)

- subword information을 이용하게되는데, 다양한 용언 표현을 서로 독립적으로 구분하여 관리한다. 

- 알고리즘 자체는 word2vec과 동일하다.

- 다른점은 주변 단어와 중심 단어를 만들 때, `n-gram`으로 나누어 학습을 수행한다. 

	- 이 때, n-gram으로 나눠진 단어는 사전에 들어가지 않고, 별도의 n-gram vector를 형성한다.

- Test를 수행할 때, 입력 단어가 vocab에 있을 경우, word2vec과 마찬가지로 해당 단어의 word vector를 return하게된다. 

- 만약 OOV일 경우, 입력 단어의 n-gram vector들의 합산을 return한다.

	![image](https://user-images.githubusercontent.com/38639633/114648428-ba48ab80-9d19-11eb-8b65-df7e7a863d5d.png)

	- 다양한 n-gram으로 쪼개서 학습한 뒤, 합산한다.

	![image](https://user-images.githubusercontent.com/38639633/114648492-dfd5b500-9d19-11eb-91f0-568f9d0fab4f.png)

	- 만일 위와 같이 OOV가 발생할 경우, 's'를 제외하고는 많은 부분이 기존에 보유한 벡터이다.
	- 이를 기반으로 새로운 단어를 유추하여 return하는 방식이다. 

- 이러한 특징으로 오탈자, OOV, 등장횟수가 적은 학습 단어에 대하여 강세를 보인다. 

	![image](https://user-images.githubusercontent.com/38639633/114648729-43f87900-9d1a-11eb-8215-f1aada69ea39.png)



**단어 임베딩 방식의 한계점**

- 그렇다면 이러한 방식으로 임베딩이 **이쁘게** 되었을까?
- word2vec이나 fasttext는 동형어, 다의어에 대하여 임베딩 성능이 매우 좋지 못하다. 
- 주변 단어를 통해 학습이 이루어지기 때문에, '문맥'을 고려할 수 없다.
	- 같은 단어를 문맥에서 다양한 의미로 사용하게 되면, 해당 단어가 제대로 학습되지 않는다.
	- 의미가 '희석'되기 때문이다. ~~이랬다 저랬다...~~
- 이에 따라서 문맥을 이용할 수 있는 언어 알고리즘이 필요하게 되었다. 



## 딥러닝 기반의 자연어처리와 언어모델

### 언어모델이란?

**모델?**

![image](https://user-images.githubusercontent.com/38639633/114649073-cb45ec80-9d1a-11eb-9e66-fa20e74278ab.png)

- 이전 state를 기반으로 미래의 state를 예측할 수 있다. 
- 미래의 state를 올바르게 예측하는 방식으로 모델 학습이 가능함



**언어모델**

- 자연어의 법칙을 컴퓨터로 모사한 모델을 말한다.
- 주어진 단어로부터 그 다음에 등장한 단어의 확률을 예측하는 방식으로 학습(이전 state로 미래state를 예측)
- 다음에 등장할 단어를 잘 예측하는 모델은 그 언어의 특성이 잘 반영된 모델이자, 문맥을 잘 계산하는 좋은 언어모델이다.



**마코프 기반의 언어모델**

![image](https://user-images.githubusercontent.com/38639633/114651553-2b3e9200-9d1f-11eb-9996-f2a42b16cbbd.png)

- 등장 빈도수를 기반으로 확률 계산을하고, 이를 바탕으로 다음 단어를 예측한다.
- 이와 비슷한 방식의 언어모델은 RNN 기반의 모델이다.



**Recurrent Nerual Network 기반의 언어모델**

- RNN은 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이룬다.

- 이전 state 정보가 다음 state를 예측하는데 사용됨으로써, 시계열 데이터 처리에 특화되어있다.

	![image](https://user-images.githubusercontent.com/38639633/114651719-722c8780-9d1f-11eb-96d2-a3df8fc25206.png)

- 또한, 마지막 출력은 앞선 단어들의 '문맥'을 고려해서 만들어진 최종 출력 vector, 즉 Context vector이며 이를 통해 classification layer를 붙이면 문장 분류를 위한 신경망 모델을 구성할 수 있다. 

	![image](https://user-images.githubusercontent.com/38639633/114651824-a30cbc80-9d1f-11eb-8959-a3263c976afc.png)

- 이 때, input 구조와 output 구조의 변경을 통해 일대다, 다대일, 다대다 구조의 신경망 모델을 만들 수 있다.



### Sequence to sequence(seq2seq)

![image](https://user-images.githubusercontent.com/38639633/114651990-f7b03780-9d1f-11eb-9399-35743733b3bb.png)

- 인코더와 디코더 구조를 통해 다양한 형태의 자연어 처리 모델을 만들게 되었다.



### Attention

**RNN 구조의 문제점**

- 입력 sequence의 길이가 매우 긴 경우, 처음에 나온 token에 대한 정보가 희석된다.
- 고정된 context vector의 사이즈로 인해 긴 sequence에 대한 정보를 함축하기 어렵다.
	- 이 문제가 가장 크리티컬하다.
- 모든 token이 영향을 미치니, 중요하지 않은 token도 영향을 준다. 



**Attention 모델**

- 인간이 정보처리를 할 때, 모든 sequence를 고려하면서 정보처리를 하는 것이 아님

- 인간의 정보처리와 마찬가지로, 중요한 feature는 더 중요하게 고려하는 것이 attention의 모티브

- 문맥에 따라 동적으로 할당되는 encode의 attention weight로 인한 dynamic context vector를 획득

- 기존 seq2seq의 encoder, decoder의 성능을 비약적으로 향상시켰다.

	![image](https://user-images.githubusercontent.com/38639633/114652368-b5d3c100-9d20-11eb-8bb5-c83dec64ee54.png)

- 하지만 여전히 `한계`가 존재하는데, RNN이 순차적으로 연산됨에 따라 연산속도가 매우 느리다. 



### Self-Attention

**Transformer**

![image](https://user-images.githubusercontent.com/38639633/114652532-02b79780-9d21-11eb-96f1-8b24b1f61f67.png)

- 순차적으로 단어가 인코딩 되는 것이 아니라, 한번에 들어간다. 







**Further Reading**

- [자연어처리](https://www.youtube.com/watch?v=jlCerj5eI4c)
- [FastText](https://www.youtube.com/watch?v=7UA21vg4kKE)
- [Seq2Seq](https://www.youtube.com/watch?v=4DzKM0vgG1Y)
- [Seq2Seq + attention](https://www.youtube.com/watch?v=WsQLdu2JMgI)

 

**Further Questions**

- Embedding이 잘 되었는지, 안되었는지를 평가할 수 있는 방법은 무엇이 있을까요?
	- WordSim353
	- Spearman's correlation
	- Analogy test
-  Vanilar Transformer는 어떤 문제가 있고, 이걸 어떻게 극복할 수 있을까요?
	- Longformer
	- Linformer
	- Reformer

 