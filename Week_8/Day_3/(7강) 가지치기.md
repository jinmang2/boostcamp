# (7강) 가지치기

**강의 소개**



본 강의에서는 딥러닝 경량화 기술 중 prunning에 대한 내용을 배웁니다.

prunning의 다양한 방법론들에 대한 내용을 다룹니다.

1. unstructured prunning
2. structured prunning

그리고 prunning을 진행하는 알고리즘에 대한 내용도 다루며 이를 실험 결과와 함께 살펴봅니다.

더 나아가 prunning에서 유명한 연구인 lottery hypothesis에 대한 내용을 다루며 강의를 마무리합니다.

 

Futher Question 

1) 이번 강의는 lightweight modeling과 어떤 관계가 있을까? (이 강의를 만든 목적은 무엇일까?

2) Mask는 언제 사용하는게 유용할까? (p 4)

3) Lottery Ticket Hypothesis의 key idea는 무엇인가? (p 28)



# Pruning for network compression

## Main idea

![image](https://user-images.githubusercontent.com/38639633/111567084-b93b6180-87e1-11eb-942e-0b4001664e0a.png)

- 덜 중요한 가지(node)들을 없애면서 모델의 파라미터를 light하게 만들기!



## Weighted sum

Decision theory에서 weighted sum model은 중요한 의미를 가진다. 

- 무언가를 결정할 때, 무게(weight)에 따라 살리고 죽일지를 `결정`한다. 
- 가중치를 둬서 우리가 어디에 얼만큼의 중요도를 가지는지를 표현할 때 많이 사용한다. 



## Pruning이란?

### 성장 과정에서의 뉴런 수의 변화

![image](https://user-images.githubusercontent.com/38639633/111567458-5e563a00-87e2-11eb-96e8-fc2a4c786bb9.png)

- 인간의 뉴런의 수는 성장기에 가장 많아지고, 성인이 되면서 많은 양의 뉴런이 없어지게 된다. 



### Pruning?

![image](https://user-images.githubusercontent.com/38639633/111567608-978eaa00-87e2-11eb-9707-7f366cb969d0.png)

- decision tree에서도 pruning 과정을 통해 중요한 가지는 남기고, 덜 중요한 것은 없애는 작업을 통해 결정 모델을 완성한다. 
- **얻는 것 :** 
	- Inference speed
	- Regularization(lessen model complexity) brings generalization - 세밀하게 튜닝되어 있던 부분들을 삭제하면서 어느 정도의 정규화 효과가 생긴다. 
- **잃는 것:**
	- Information loss
	- The granularity affects the efficiency of hardware accelerator design - 네트워크의 웨이트를 잘라낼 때   세밀한 정도(granularity)에 따라서 하드웨어에 옮길 때 옵티마이즈 관점에서 까다로워진다. 



### Deep learning 에서의 pruning

![image](https://user-images.githubusercontent.com/38639633/111568515-3e277a80-87e4-11eb-8276-d5162d1750a9.png)

> [Learning both Weights and Connections for Efficient Neural Networks](https://papers.nips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf) 

- 위의 그림을 보면 좌측은 pruning 전, 우측은 pruning 후이다. 
- 비교해보면 전체 스케일이 다른 것을 확인할 수 있다. 
- 0을 주변으로 많은 웨이트들을 잘라내어 우측의 이미지로 된 것을 확인할 수 있다. 



### Pruning & Dropout

- **유사점 :** pruning과 dropout은 가지치기를 통해 연결을 끊고 잇는다는 점에서 유사한 점을 가진다. 

- **차이점 : ** Pruning은 한번 연결을 끊으면 다시 연결하지 않는다. 반면에 dropout은 껏다 켰다를 반복하면서 학습을 진행하고, inference를 진행할 때에는 다 연결된 상태의 네트워크 상태로 진행한다. 

	> [https://arxiv.org/pdf/1503.02531.pdf](https://arxiv.org/pdf/1503.02531.pdf)
	>
	> [https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)   

- ![image](https://user-images.githubusercontent.com/38639633/111575330-74b7c200-87f1-11eb-8641-69256582783f.png)
- 위 그림처럼 iterative하게 pruning을 실행한다. 중요한점은 앞서 설명한 것과 같이, 연결을 잇거나 끊으면서 앙상블하는 방식은 아니다. 
- ![image](https://user-images.githubusercontent.com/38639633/111575922-c4e35400-87f2-11eb-8a0b-487b62ab23ad.png)
- 드롭아웃의 경우, 프루닝과 달리 많은 시도를 통해 앙상블하면서 결과를 좋게 유도한다. 





