

# P stage 4 DKT 2주차 화요일 오피스 아워 - DKT 논문 톺아보기 및 기타 팁 제공 - 박현병 멘토님

일감호 오리 멘토님 (힘을 숨긴 취준생)

## Kaggle 대회 1등 솔루션 (Last Query Transformer RNN)

- 6강에서 배운 부분을 좀 더 자세히
- 이 모델은 Encoder만 사용
- Positional Encoding이 없다는 점을 주의할것!
- Encoder는 커스텀되었는데, self-attention 쪽에서 쿼리의 마지막 벡터만 사용해서 셀프 어텐션을 계산하기 때문에 이런 이름이 붙음
- feature engineering이 일절 사용되지 않음(…)
	- 오로지 모델에게 모든것을 의존했음. 이외의 모든 상위권 솔루션은 피쳐엔지니어링을 사용함(!!)
- Max Sequence Length가 길어질수록 validation AUC가 상승
	- 그런데 Max Sequence Length의 길이는 Attention 계산량의 문제에 부딪히게 된다.
		- Attention 계산량은 MSL의 제곱에 비례
	- 이걸 해결하기 위한 방법으로, 맨 마지막 문제의 정답 여부만 고려하므로 query의 마지막 문제 벡터만을 사용!(last query) -> O(n)이 걸리게 됨.
	- 이 방식으로 계산량 문제를 해결하여 MSL을 1728로 아주 길게 설정할 수 있었음.
- Skip Connection의 일반적인 Add 연산은 2D
	- last query vector는 1D로 나오므로, broad casting하여서 더해주었음.
- Last Query를 사용할 때 주의할 점
	- Last Query는 Prepadding일 경우 코드 한줄만 바꾸면 구현 가능
		- 일반적인 쿼리에서 마지막 쿼리만 가져오도록…
	- 근데 문제는 PostPadding인 경우. 이 경우 마지막 쿼리의 인덱스가 매번 다 다르기 때문… 마지막 인덱스 말고 중간 인덱스 중 하나를 가져와야함.(뒤쪽은 padding)
	- Attention Map 사이즈에 맞춰서 1D처럼 생긴 2D mask를 생성해야함
- Baseline은 prepadding으로 제공되어있는데, masking 코드가 나중에 제공될 예정이긴 하지만 직접 꼭 구현해보셨으면 좋겠습니다
	- Attention에서 masking이 어떻게 이루어지는 지 정확히 이해못한다면 어려움이 있을 수 있어요.
	- 1D같은 2D 마스킹을 직접 만들어보시길 권장드립니다.
- Padding이 앞에 되어있는데, 굳이 Masking을 해야하나?라는 질문이 있을 수 있는데, 만약 Masking이 없으면 어떻게 될까요?
	- Masking을 할 때에는 Padding이 제외되지만, Masking이 없으면 softmax 계산 시에 Padding 값 및 갯수에 Softmax계산이 영향을 받게 됩니다.
	- **따라서 꼭 Padding이 있다면 Masking을 하기를 권장드립니다!**
- Last Query라는 커스텀 모델을 만들경우는 어떻게 제작해야할 지 감이 잘 안올 수 있을 것 같아서, Masking이 무엇이고 Mask를 어떻게 만들어야하는지 시각적으로 설명하려고 합니다(슬라이드)
	- Masking을 하지 않으면 1번 문제를 풀면서 2번 문제를 미리 엿볼 수가 있습니다.
	- 데이터가 있는 부분을 0으로, 마스킹 하고싶은 부분을 1로 설정
		- Huggingface에서는 데이터가 있는 부분을 1로 설정하던데요?
		- -> Huggingface도 내부 구현에서는 0과 1을 바꾸어서 저장해줍니다.
		- 다만 Pytorch Transformer는 거꾸로 설정되어있긴 해서… 프레임워크 맞추어서 사용하시면 됩니다.
	- 가리고 싶은 부분인 Mask의 1을 -inf(사실 그냥 1로 두어도 PyTorch 모델이 알아서 -inf로 바꾸어줌)로 바꾸고, 가리고 싶지 않은 부분의 Mask 0을 0.0으로 바꾼다.
	- 이 상태에서 softMax연산을 취하게 되면 -inf 부분은 0으로 바뀐다.
		- softMax 계산시 e−infe−inf가 0으로 수렴하기 때문.
	- Attention Map에서 Masking 되지 않은 부분 X Value의 해당 행을 곱하여 Output이 나오게 된다.
- PyTorch `nn.MultiheadAttention`은 2D/3D 마스크를 입력으로 받는다.(지금 DKT에서 사용하는 Attention Map)
	- Head 개수만큼 query weight가 concat 되어서 Input Tensor x Query Weight = Query를 형성.
	- (이 부분은 슬라이드와 영상을 보시는 게 훨씬 잘 이해가 되실 것 같아서 따로 적지 않겠습니다)
	- 2D Mask는 각각의 Attention Map에 broadcasting되어 더해지게 된다.
	- 그러나 문장마다 마스크가 다른 경우 2D Mask를 사용할 수 없다.
		- 이때 사용하는 것이 3D Mask.
		- PrePadding에서 Masking을 한다면 아마 Mask 부분이 다 달라서 3D Mask를 사용해야한다.
		- 다만, Last Query를 사용한다면 Query를 하나만 사용하여 Tensor 사이즈가 다르기 때문에 3D Mask 만들 때 주의하자.

## 가중치 초기화(weight Initialization)

입력갓이 너무 커지거나, 너무 작아지지 않고 제대로 전파되고 있는가?

- 입력값에 가중치가 계속 곱해질때, 가중치에 어떤 값을 넣느냐에 따라 입력값은 커지거나, 유지되거나, 감소한다.
- 적절한 가중치를 선택하지 않으면 입력값을 매우 커지거나, 매우 작아진다.
- 가중치로 인해 입력값이 매우 커지면?
	- Sigmoid함수를 activation 함수로 사용시 Gradient Vanishing 현상이 일어남
		- 입력값이 5만 되어도 Gradient 값이 거의 0…
	- 딥러닝에서 sigmoid를 사용시 Nested Logistic 회귀이기에, logistic 회귀 특징을 이어받는다.
		- 입력값 간에 차이가 크게 벌어져서, 조건수(Condition Number)가 증가한다.
		- 이 경우 가중치 값이 입력값의 변화의 매우 예민해져 Overfitting된다.
	- 즉, 모델이 불안정해진다(학습이 제대로 이루어지지 않는다)
- 가중치로 인해 입력값이 매우 작아지면?
	- BackProp시에 Weight에 주어지는 Gradient 값이 0에 가까워져서, 최종 출력에 가까운 가중치일 경우 거의 변하지 않고, 모델의 앞부분부터 천천히 회복한다.
		- 따라서 회복까지 시간이 너무 오래걸려 학습이 느려진다.
- 따라서, 곱하기에서의 1과 같이 입력값을 적절히 유지하는 가중치로 초기화해야한다.
	- 그럼 행렬에서는 가중치를 1이 아니라 어떤 형태로 초기화해야 적절히 유지가 될까?
		- 활성화 함수에 따라 다르다.
		- 첨부해주신 colab 코드 참조!
- [tanh - Xavier 초기화]
	- 벡터 길이를 통해서 scaling 값을 구한다.
	- 완전히 해결되는건 아니지만 입력값을 적절히 정규분포 모양으로 scaling 해준다.
- [ReLU - Kaming 초기화]
	- ReLU와 Xavier는 궁합이 맞지 않음(제대로 초기화되지 않음)
	- ReLU는 Kaming을 사용할것
- 근데… 곱하기만 되지 않는다면…?
	- 곱하기 중간에 새로운 계산 과정(ex-더하기)이 들어간다면 더는 가중치 1로 입력값을 유지할 수 없다.
		- Skip Connection이 곧 더하기의 상황이라고 볼 수 있다.
		- 가중치를 새로운 상황에 맞추어 바꾸어야한다.
	- 즉, 가중치 초기화는 정해진 규칙이 있는게 아니라 상황에 맞추어서 수행해야한다.
		- [ReLU-Kaving], [Tanh-Xavier]인것은 FC Layer만 존재하는 모델에 한정된 이야기이다. 즉 가이드라인만 제시할 뿐이지, 모델이 바뀌면 모델에 맞추어서 가중치 초기화 방법을 적절히 바꾸어줘야한다!
	- 이 파트도 Colab 코드 참조!
	- 그래서 pytorch의 `xavier_uniform_` 같은 함수에 parameter로 Gain이 있는것(상황에 맞추어서 가중치 크기를 바꾸기 위하여)
- 근데 우리는 지금까지 가중치 초기화 제대로 안했는데 잘만 학습시켰는데요?
	- BatchNorm과 LayerNorm 때문!
	- Transformer에서 Norm 과정을 빼어버리면 성능저하가 확실하게 생긴다.
		- 성능 저하가 매우 심각할수도 있다.
- 그럼 Normalization은 무조건 넣어야하나요?
	- 그건 또 아닙니다.
	- 단점
		- 계산 비용이 비쌈.
		- Batchnorm의 경우 Gradient Exploding 현상 위험
		- LayerNorm의 경우 입력값이 커지면 커질수록 Gradient Vanishing이 일어날 확률이 높아진다.
- 그럼 Normalization을 쓰지 않고 훈련이 가능한가요?
	- 네, 가능은 합니다.
	- 다만 지금까지 들었던 것 처럼 가중치 초기화를 아주 섬세하게 해주어야합니다.
	- ==Fixup(fixed_update initialization)==이라는 방법론이 그래서 나옴.
		- 기존 가중치 초기화 방법 사용하되, 입력값 X는 fixed하고, 가중치 scaling을 통하여 전파되는 X값의 크기 조절
		- 가중치 초기화를 수행하는 방법론을 제시
		- T-Fixup(Transformer Fixup)도 관심있으신 분은 찾아보세요.
- 다 귀찮으면… 그냥 Normalization 쓰세요.
- 오늘 데일리 미션이 gradient 변화를 살펴보는 건데, norm 빼보고 초기화 한후 보시면 gradient가 난리나는걸 보실 수 있을거에요!

## DKT 논문 톺아보기

- 지금 대회에서는 지식상태보다는 주어진 문제를 맞췄는지, 틀렸는지에 집중한다.
- 근데 DKT를 정확하게 모델링하려면 지식상태를 예측해야함.
	- 입력에 대해서 해당 입력의 지식상태를 출력하고, 이 출력값이 높음(잘학습됨), 낮음(잘 학습되지않음)에 따라서 틀릴지 맞힐지 예측하는게 정석이긴함.
	- 근데 캐글 솔루션도 그렇고 사실 다 정답 맞췄는지 예측하는 방향으로 가고 있다(…)
- 지식상태를 알 수 있다면, 어떤 순서대로 문제를 풀었을 때 학습에 가장 효과적인 지 예측할 수 있다.
- 지금같은 대회가 아니라, 학생이 각 지식을 최대한 이해하는 것을 목표로 한다면?

### BKT(1994)

- 지식 상태를 예측하긴 하는데, 한번 배우면 절대 까먹지않는다는 가정이 있어서 현실적이지는 않음.
- 지식의 이해도를 각각 분석하기 때문에, 각 지식간의 연관성을 파악할 수 없음.
	- 더하기를 풀었을 때 곱셈의 숙련도도 올라갈 수 있는데, 그런것은 고려하지 못함

### DKT(2015)

- LSTM 모델을 이용하여 최초의 지식과의 연관성을 고려하기 시작
- 우리 모델은 문제가 주어졌을 때 해당 문제의 정답 여부를 예측하지만, 2015 DKT는 문제와 정답여부가 주어졌을 때 다음 문제의 정답 여부를 예측.
- 우리 모델은 Question과 Response로 분리하지만, 2015 DKT는 Interaction이라는 이름으로 합쳐서 관리
	- 그때는 그게 더 성능이 좋았다고 함.
- 두 방법 모두 성능 차이는 없기때문에, 수행하려는 task에 따라 방식을 달리 할 것.
- DKT에서 예측된 지식상태는 매 문제를 풀때마다 변한다. 또한 문제 풀이 정보가 많이 입력된 후반부일수록 지식 상태 예측이 더 정확해진다.(초반부의 지식상태는 정확하지 않다.)
- 그리고 시간이 지날수록 지식을 까먹기도 한다.(LSTM 특성상)
- 그러나 2015 DKT의 문제는, 지식상태를 예측하는 방식이 학생이 학생이 푼 문제지를 채점하면서 채점하는 매순간 학생의 학업 수준을 평가하는것과 같음.
	- 어려운 문제를 풀면 오!! 천재네!! 하다가 쉬운 문제 틀리면 얘 사기친건가?? 하다가 다시 어려운문제 맞추면 오!! 천재네!!!(…)
- 첫번째 문제로 i 문제를 풀었을 때 j번째 문제의 정확도 확률을 추산함.
	- 그래서 문제간의 영향력(A문제가 B문제에 가지는 영향력)을 파악할 수 있음.
	- 논문에서 문제간의 영향도를 클러스터링하여 그린 그래프도 제시했음.

### DKT+(2018)

- DKT의 문제점 보완
	1. 입력된 문제가 맞았는데도 해당 문제 이해도가 떨어지는 현상
		- 나누기 문제를 방금 맞았는데 이해도가 오히려 떨어짐.
		- 왜냐하면 DKT 모델은 다음 문제인 곱하기를 예측하는데에만 신경쓰고 있기 때문.
		- **현재 문제의 정답을 정확히 예측하는지도 loss로 넣어 이를 보완.**
	2. 학생의 예측된 학습수준이 시간의 흐름에 따라 일관되게 변하지 않는 현상
		- 감정기복 심한 선생님은 옳지 않다.
		- 모든 시간대에서 인접한 두 문제의 차이를 모두 더해서 정규화하여 시간흐름에 따른 학습 수준의 변화 변화를 최소화시킨다.

### SAKT(2019)

- Transformer가 DKT에 최초 적용된 사례
- 이 시점부터는 지식상태에 대한 이야기가 나오지 않음. 문제 정답을 예측하는데에 초점이 맞춰짐(auc 스코어)

### SAINT(2020)

- Interaction은 question과 response로 분리하여 입력
- 문제를 푸는데 다양한 feature를 사용

### SAINT+(2020)

- 기존 SAINT에 feature 2개 추가
	- Elapse Time(문제를 푸는데 걸린 시간), Lag Time(한 문제를 끝내고 다음문제를 풀기시작하기 까지의 시간?)
	- 시간차가 적으면 categorical embedding, 아니면 nemerical embedding

------

## 라이브 QA

- 제공된 lastquery 코드 주석에는 마스크가 필요 없다고 적혀있었는데 잘못 적혀있는건가요?
	- 혼란을 드려 죄송합니다. 제가 그 코드를 작성할 때 필요가 없다고 생각을 잠깐 했었는데, **마스크 필요합니다!**
	- prepadding 마스크 구현하실때 tensor 모양 고민해야하기때문에 어려우실텐데, 이 참에 transformer 구조를 좀 더 이해하신다고 생각하시면 좋을것같습니다 ㅠㅠ
- 지식 상태를 예측하는 모델을 학습할 경우에 초기의 지식 상태는 어떻게 설정하는건가요?? 제일 처음에는 학습자의 지식 수준을 낮은 상태로 설정해야할것같은데, 그런 초기 설정은 어떻게 되어있나요?
	- 제 생각에는 초기에 지식 상태 Loss 함수에 정규항을 추가시켜서 penalty를 만들면 초기상태의 지식수준을 적당히 설정할 수 있지 않을까 합니다.
- 최근의 논문들이 학생의 지식상태보다 다음 문제를 맞췄는지 틀렸는지에 중점을 두는 이유가 특별히 있는것인지 궁금해요!
	- 특별한 이유가 있는것 같진 않아서 정확히 답변드리기는 어려울것같아요.
	- auc를 상승시키고 싶다는 생각에 이런 논문들이 나오지 않나 하네요.
- upstage 발표 보고 시도했을땐 우리 데이터셋은 큰 성능 개선은 없었습니다.
	- 저희 데이터셋에서 max-seq-len을 늘렸는데 성능 향상이 크지 않았다면, 그건 사실 데이터셋 특성입니다.
	- 그래서 데이터셋 특성을 먼저 이해하고 맞춰서 방법을 사용하셔야 할 것 같습니다.
- 혹시 시간의 흐름에 따라 지식을 까먹는다는 가정을 하고 지식상태를 마이너스하는 것에 대한 것은 시도할만한 가치가 있을까요?
	- 말씀하신 그 가정을 가진 모델이 있었거든요.
	- 아마 뤼이드 테크 블로그에 들어가보시면 그 모델이 소개되어있을거에요.
	- 할만한 가치는 있다고 생각합니다! 뭘 하든간에 저희는 지식의 영역을 넓혀가는 중이니까, 어떤 게 좋은지는 해봐야 알지 않을까요?
- 아까 지식상태에 대해서 문제수로 늘리면 된다 하셨는데, 그럼 지식상태에 대한 gt는 어떻게 정해야할까요? 그 문제에 대한 최대값?
	- 슬라이드가 조금 오해를 일으킬 수 있는데, 정확히 말하면 ground truth는 해당 그 문제에 대한 loss입니다. 지식 상태는 모델에 학습되는것 뿐이지 출력에서 영역별로(ex-곱하기, 나누기,더하기,빼기)별로 지식 수준이 숫자로 나온다던가 그런건 아닙니다
- user에 대한 지식상태 뿐만 아니라, 문제(item)의 특징도 매우 중요하다고 생각이 듭니다. 사칙연산을 예시로 들면, 해당 문제가 어떤 유형에 속하는지, 어느정도의 난이도를 가지는지 등등을 표현해줘야 할 것 같습니다. 이에 대한 특징은 feature engineering을 통해 만들어야 할 것 같다고 생각합니다. riiid의 1등의 솔루션은 이런 feature engineering이 없는데, 이와 같은 정보를 단순히 문제 id로 구별하고 등장하는 패턴으로 학습을 했다고 이해하면 될까요? 마찬가지로, user의 지식 상태도 user의 id만 입력된 것 같은데, 동일한 맥락일까요?
	- 저희 대회는 user id 입력하지 않고, 문제 id만 입력시킵니다.
	- 유저ID를 쓰는 논문이 나왔던거같긴한데…
	- 뤼이드에서는 feature engineering이 없긴했었는데, 실제로는 feature engineering을 해주어야 좀더 모델이 원활하게 할 수 있을겁니다.
		- 뤼이드 대회 1등 솔루션은 좀 특이한 경우죠. 모델이 직접 feature engineering을 수행하고 거기서 좋은 정답을 찾도록 했던 거라서…
	- 그리구 뤼이드 데이터는 1억건이나 되는 데이터가 있어서 모델이 더 좋은 feature를 잘 찾아낼 수 있지 않을까 합니다.
		- 저희는 그에 비해서 훨씬 적은 데이터라서, 이번 대회는 feature engineering을 하는게 더 좋을것 같긴 합니다.
	- (질문) 그럼, 한 유저의 시퀀스를 기반으로, 해당 유저에 대한 지식상태를 추정한다고 이해하면 될까요? item에 대한 특징 정보는 전체 data를 통해서 학습해 패턴을 찾아내는 것일까요?
		- 네 맞습니다. 유저가 누구인지 특별히 명시하진 않는거잖아요?
		- Item의 특징 정보는 embedding layer를 쓰는거잖아요? 학습을 하다보면 자연스럽게 각 문제에 대한 vector 값을 학습한다고 생각하시면 될 것 같습니다.
			- Embedding layer에 문제의 패턴이 들어가있다고 생각하시면 될것같아요.
			- Embedding layer를 가지고 시각화해서 문제를 clustering하는 논문도 있었어요.
- DKT 관련 질문은 아닌데, PPT를 정말 잘 만드셨다고 생각하는데 이에 대한 꿀팁 같은게 있을까요? 한땀 한땀 만드시는건가요…?
	- 저 원래 디자인쪽 전공을 해서…
	- 몇개월 배우시면 되지 않을까요…? ㅎㅎ
- 혹시 DKT는 부스트캠프 때문에 공부하기 시작하신건가요?
	- 네 저도 부스트캠프때문에 공부한건 맞구요. DKT가 생소한 분야거든요…
	- 저도 전문가는 아니라 부끄럽긴 하네요…!!
	- 제가 말씀드린 부분들이 뇌피셜이 있어서 틀린게 있을수도 있습니다…!

------

edwith에 자료가 다 올라갈 예정입니다.
prepadding masking 꼭 구현해보시면 좋을 것 같습니다!
질문 있으시면 언제든 연락주세요!

Select a repo

Subscribe