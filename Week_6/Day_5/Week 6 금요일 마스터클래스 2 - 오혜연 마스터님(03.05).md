# Week 6 금요일 마스터클래스 2 - 오혜연 마스터님(03.05)

카이스트 전산학부 교수님

## 사전학습

- 마스터님께서 인공지능 윤리에 관심을 가지게 된 계기가 궁금합니다. 전산학 박사과정을 밟으시면서 하게된 특별한 경험이나 박사과정을 하겠다고 결심하게 된 이유도 알려주실 수 있으실까요?
	- 석사 이후 연구소에 있었는데, 연구소에 있다보면 박사를 해야겠다는 생각을 하게 됩니다(주위 사람들이 다 박사라서)
	- 그래서 그냥 갔다왔어요.
		- 그래서 MIT 박사로 가셨다고…
	- 딱히 계기가 있어서라기보다는… 그냥 인공지능이 끼칠 영향을 생각하다보니…
- 마스터님께서는 인공지능이 전문가(ex. 법조인, 개발자 등등)를 대체하기까지 예상되는 시간이 얼마나 걸린다고 생각하시나요?
	- 대체할수는 없다고 생각합니다.
	- 그럴수도 없고, 그래서도 안되게 열심히 노력해야한다고 생각해요.
		- 법조인, 개발자, 교수도… 모두.
	- 사람이기 때문에 더 잘할 수 있는게 있겠죠?
		- 교수로 치자면, Teaching이 정보를 전달하는 것뿐만이 아니잖아요.
			- 감동을 주고, 유대감을 형성하는 등의 모든 것들이 동반되어야하니까…
			- 그런 노력을 해야할것같아요.
- 자율주행 상황에서는 윤리적으로 정답이 존재할 수 없는 상황이 많은데요. 이런 경우처럼 답이 없는 경우에 어떤 식으로 모델을 설계해야 할까요?
	- 이건 많은 고민/연구/다자간의 대화가 필요한 일인 것 같아요.
	- 누군가의 삶과 기물 등의 가치가 파괴되는 것을 말씀하시는것 같은데…
	- 모델을 설계하는 사람한테만 이런 상황에서 어떻게 해야할까?를 질문할 수는 없는 것 같아요.
		- 관련된 모든 사람들이 objective function을 어떻게 정해야 할지에 참여한다고 생각해요.
	- 좋은 답은 아닌것같네요. '간단한 답은 없다’고 생각합니다.
- 거시적으로 봤을 때, AI Ethics 연구의 원동력은 기업의 리스크 관리가 대부분인가요? AI Ethics이 equality, privacy 등의 목표를 달성하기 위해 모델의 성능을 낮추거나 cost를 많이 높인다면 돈이 벌리는 분야보다 기술 발전이 더딜 것 같습니다.
	- 현실적인건 리스크관리가 맞죠.
		- 누군가 소송을 하지 않게, 소송을 하더라도 기업이 책임을 지지 않게…
	- 그렇지만 AI를 연구하고 개발하는 기업이라고 하면, financially/legally 사용할까만 아니라, 정말 좋은 기업이라면 우리 사회에 어떤 가치를 창출해야하는가에 대해 고민을 많이할거라고 생각해요.
		- 그런 관점에서 봤을 때 AI를 하는 기업은 정말 포텐셜이 큰거죠. 사회에 큰 영향을 끼칠 수 있으니까.
		- 사회가 가지고있는 난제들을 해결하겠다는 마음을 가져야하지 않을까요?
	- Ethics를 생각하면, 기술발전에는 걸림돌이 되지 않을까 하는 말을 많이들어요.
		- 연구자들로서는 그런 생각을 할 수 있다는 생각을 해요. 성능을 높이는것과 윤리와는 상충되는것이 아닐까?
		- 정확도를 높이기 위해서는 큰 모델이 되어야하는데, 큰 모델을 만들려면 리소스가 너무 많이 필요하고, 기업간의 규모에 따른 불평등도 커지고…
			- 그래서 구글같은 leader들이 이런 상충되는 문제(ethics,환경파괴,불평등 vs 성능)들을 많이 고민하고, 주도적으로 끌고 나가야한다고 생각해요.
- 정도의 차이는 있겠지만, 어떠한 기술 개발 및 서비스를 개발하는 과정에 있어 윤리적인 이슈가 터질 위험이 높다고 가정할 때, 윤리적 이슈 해결이 먼저일까요? 아니면 일단 기술을 디벨롭 시키면서 동시에 혹은 이후에 윤리적 이슈를 해결해야 할까요? 윤리적 이슈로 인해 개발에 들어가기 이전에 무산되거나 개발 과정에서 중단된 사례를 경험하시거나 보신적이 있나요?
	- 지금까지 개발된 걸 보면 윤리적 이슈를 늦게 고민한거죠. 여러 사례들을 보면…
	- 저는 피해보는 사람들이 있으니 그게 꼭 맞다라고 생각하진 않는데,
		- 어떻게 보면 예측은 할 수 있었으나 막지는 못하는 그런 부분들도 있다고 생각해요.
		- 기술들이 발전될 때, 윤리적 이슈에 대해 미처 생각하지 못하고 기술만 먼저 개발이 되니까 피해자가 생기죠.
		- 그걸 하지 않으려고 해야하지만, 기술을 만드는 사람으로서는 그게 굉장히 어려워요.
			- 언어모델을 만들면서 ‘편향이 있을 수 있으니까 만들지 말자’, 이건 현실적으로 어렵죠.
			- 프로덕트(서비스)로 나와서 사용자에게 서비스되는 거면 좀 더 그런걸 많이 봐야하지만, 그래도 쉽지는 않죠.
- AI 시스템에 의해 법적인 문제가 발생했을 때 개발자의 책임은 어디까지일까요?
	- 저는 법쪽 전문가는 아니라서 법적인 책임은 잘 모르겠습니다…ㅎㅎ
	- 만약 이걸 알고있었는데 해결하려고 노력하지 않았다? 라면…
		- 개발자/연구자로서의 도의적 책임의식은 가지고 있어야한다고 생각해요.
		- 이루다의 사태도 그렇고, 자기가 공부하지 않았으면 책임이 있다고 봅니다.
		- 법적인 책임은 모르겠지만, 최소한 마음은 무거워야 하지 않을까.
		- 그리고 전 연구자로서 학교에 있으니까, 그렇게 가르친 교육자들도 책임을 느껴야하겠죠.
- KAIST에서 국방부, 한화시스템과 함께 AI무기개발을 한다는 뉴스를 2018년에 본거 같습니다. 저는 그 이후에 AI무기개발에 대한 소식을 듣지 못했는데요. 계속 진행중일까요? 아니면 윤리적인 부분을 고려해서 방향을 바꾸어 개선중일가요?(비살상무기, 정찰용)
	- 저는 그 프로젝트에 관여하지는 않았구요. 그 프로젝트가 계속되고 있는지도 잘 모르겠어요.
	- 그렇지만 카이스트에서는 윤리서약같은 것을 냈어요.
		- 인류를 해치는데 쓰이는 인공지능 기술을 개발하지 않겠다라는 내용의.
	- 그러나 한화와 하는게 중단되었을지도 모르지만, 카이스트가 국방 관련된 개발은 계속 하고있다는 걸로 알고있어요.
		- 예민한 이슈죠. 국방 기술이 방어체제일수도 있지만 공격 체제일수도 있고…
		- 국가에 국민의 예산이 어떤식으로 쓰이는지 조금 더 관심이 많아야하는데…
		- 굉장히 많은 공대의 연구가 국방에서 나오는건 맞는것같아요.
- AI 도입으로 인해 일자리 경쟁에 대한 교수님의 생각이 궁금합니다. 예를 들어 택시기사분들과 자율주행을 연구하는 개발자들에서 택시기사분들의 요구를 들어주면 기술에 대한 국가경쟁력이 약해지고 개발자의 입장에 손을 들어주면 많은 일자리를 뺏기게 될텐데 저는 이런 상황에서는 인간과 AI간의 경쟁을 시키는 것 밖에 없다생각합니다. 택시는 서비스를 AI쪽은 편의성(?)으로 서로 경쟁하게 놔두고 법안으로 하나를 금지 시키는것은 좋은 생각이 아니라고 생각하는데 교수님의 의견이 궁금합니다.
	- 저는 택시의 경우든 무슨 경우든 AI랑 사람이 경쟁한다는건 이상한 것 같아요.
	- AI의 홍보가 뭔가 이세돌vs알파고처럼 경쟁구도로 가서 AI가 사람의 경쟁상대인것처럼 잡힌것같은데,
		- 전혀 경쟁상대가 아니라 오히려 협력 구도라고 생각해요.
		- 모빌리티에서도 AI와 사람이 어떻게 협력을 해야할까라고 생각합니다.
- 사고없는 완벽한 자율주행을 위해서는 중앙정부에서 모든 차량을 AI기반으로 통제해야한다는 의견을 들은 적 있는데, 이에 대해 어떻게 생각하시는지 궁금합니다.
	- 지금 사람들이 꿈꾸는 정말 모든 분야에서 인간같은 AGI는 좀 먼 미래의 일인것 같구요.
	- 그보다는 한 두개의 task를 정말 잘하는 AI는 있는데, 그런것들이 조금씩 세상에 대해 알아가면서, 상식적인 부분까지 이해하면 어떤 task에 대한 AGI는 나올것같고요.
		- Physical context도 조금 다를 수 있는데,
			- 예를 들어 파파고도 컴퓨터에서 하는 것과 현지인과 현지에 가서 바로 번역하는것은 다르겠죠.
			- 그런 것까지 임베딩된 파파고가 나와야 할 것 같아요.
	- 두번째로는, 사람들은 개인별로 다 다르잖아요.
		- 성격/말하는 어휘/어투/성장…
		- 사람처럼 한명 한명이 되고 세상에 대해 많은 걸 알고 많은 걸 할수 있는 AGI는 지금은 아직 현실성이 없는것같아요.
			- 불가능하다기보다는, 제가 죽기전엔 안나오지 않을까…ㅎㅎ
			- 만약 그렇게 되면 윤리적이슈도 너무 많구요.
- 모델이 편향을 갖게 되는 이유가 현실 세계가 이미 편향되어 있고 그로부터 얻은 데이터를 학습했기 때문이라고 생각하는데, 그렇다면 편향을 없애기 위해서는 학습 데이터에서 편향을 없애는 작업을 하는지 아니면 학습 후에 모델의 아웃풋을 조정하는지 궁금합니다.
	- 일단 먼저 말씀드리자면 둘 다 합니다.
	- 그러나 편향이라는게 그렇게 쉽게 정의내릴 수 있는건 아니에요.
		- 쉽게 판단할 수 있는것
			- 의사는 항상 영어로 번역하면 he가 되고, 간호사는 항상 영어로 번역하면 she가 된다 -> 나쁜 편향이다!
			- 흑인에게는 criminal, 백인은 nice -> 나쁜 편향이다!
		- 그러나 어려운 것들(최근에 연구실에서 연구하고 계시다고 합니다)
			- 일본인은 적이다 -> ??
			- 일본인은 스시를 좋아한다 -> 정말로 다 그런가?
			- 한국인은 김치냄새가 난다 -> 그게 좋은건지, 안좋은건지… 사람마다 다르겠죠?
	- 또, 욕설도 그럴수 있겠죠.
		- 세대별로 정의하는 욕설이 다를 수 있어요. 40대에서는 욕설이라고 생각하는 말이 10대 20대에서는 아닐수도 있죠.
		- 이런 걸 전부 없애버리는 건 어떻게 생각하면 굉장히 위험할 수 있어요.
		- 언어모델은 다른 task를 하기 위해서 만드는거잖아요.(번역, 감정분류 등…)
			- 언어모델이 동작하는건 데이터 패턴에 있는걸 잘 넣어서 downstream task를 하게 되는건데, 편향을 없애게 되면 사실 downstream task를 잘 못하게 될 수 있어요.
				- 물론 없애면서 downstream task가 잘 안되는지 검사하긴 하는데…
	- 그래서 학습 input을 없애거나, output을 없애거나 하나만 딱 사용하자고 말씀드리긴 어렵다는게 제 입장입니다.
- 큰 회사에서는 인공지능 모델 개발과정에서 윤리학자까지 고용하는 것으로 알고 있습니다. 완성된 인공지능 모델의 윤리성이나 합법성은 어떻게 측정하나요? GLUE 같은 점수 지표가 있거나 나올 가능성이 있을까요? 개인 프로젝트나 작은 회사에서는 어떻게 윤리성을 고려하거나 할 수 있을까요? 규모가 작은 개인 프로젝트나 소규모 회사에서는 어떻게 윤리성을 고려하나요?
	- '아직은 없다’는게 답입니다. 그러나 지금 벤치마크들이 조금씩 나오고 있어요. 아직 제대로된 벤치마크는 없긴 합니다.
	- 윤리성을 측정하는것과 합법성은 다른 일이에요.
		- 법은 나라마다 다르고, 인공지능 윤리에 대한 법은 유럽이 좀 더 잘되어있기도 하죠.
	- 윤리성이 아니라 편향에 대한 지표를 측정하는 metric이라고 한다면
		- 있어요. WEAT같은것.
		- IAT(Implicit Association Test)
			- 사람의 편향에 대해 체크하는 테스트인데, 이걸 워드임베딩이나 sentence에 대해서도 해요.
		- 확률에 대해서
			- ex)‘일본인은 나쁘다’ 와 '한국인은 나쁘다’라는 두 말이 나올 확률을 따졌을때 지표가 많이 다르면 문제가 있다고 할 수 있겠죠. 최근에 제시된 방법이긴 하지만.
	- GLUE같은게 나올 가능성은 당연히 있습니다.
	- 규모가 작은 회사에서는
		- 테스팅을 그냥 엄청 많이 해봐야한다고 생각하구요(개발하면서, 개발한 후에)
		- 그걸 내부적으로만 하지 않고, 나랑 다른사람들이 해야한다고 생각해요.
			- 이루다같은게 이슈화가 된 이유가 챗봇을 가지고 다양한 사람들에게 써보게 하면서, 그 질문에 대해 어떤 답변이 나오는지 충분히 테스트 해보지 않았기 때문이라고 생각해요.
		- 실제로 이걸 출시해보겠다라고 하면, 10대부터 70대까지 남녀노소, 외국인, 여러 분야의 직종들에게 테스팅을 해야하지 않을까요?
			- 물론 규모가 작다면 사실 돈도 없으니까 그걸 어떻게 하냐…라고 할수 있겠지만…
			- 그정도는 노력을 해야한다고 생각합니다.ㅠㅠ
- 유투브를 시청하다보면 컨텐츠를 정말 잘 추천해준다는 생각이 듭니다. 하지만 한편으로는 추천 시스템에 의해 ‘내 취향이 만들어지는게 아닐까?’ 라는 생각도 드는데요. 이에 대해 어떻게 생각하시는지 궁금합니다.
	- 지금은 벌써 인공지능과 더불어 사는 세상이 됐죠.
	- 우리가 AI를 개발하지만, AI도 우리 삶에 영향을 끼치죠.
	- 그렇지만 그건 인공지능이 특별한게 아니고 자동차, TV 등의 모든 테크가 다 저희한테 영향을 주는 거죠.
	- 그건 어떻게 생각을 해야하기보다는, 당연한 fact가 된거고, 어떻게 잘 더불어 살수있을까의 문제일것 같습니다.
- 인공지능에 의해서 생성된 얼굴 이미지가(설사 입력데이터에 포함되어있지 않더라도) 실제 존재하는 사람 혹은 미래에 태어날 사람의 얼굴과 동일(혹은 사람이 인식하기에 동일)할수 있는데 이것에 대한 윤리적인 문제는 없을까요?
	- 이렇게 된다는 것 자체에 윤리적인 이슈가 있다기보다, 이걸 가지고 뭘 하냐의 문제일 것 같아요.
		- ex) 대통령의 얼굴로 편향적인 헤이트스피치를 하는 경우
	- 그 자체가 문제라기보다는, 기술을 만드는 사람들이 윤리적인 문제가 생길 수 있음에 대한 인식을 하고있어야한다고 생각해요.
		- 이게 어떻게 쓰일 수 있을지.
- Deepfake, 3D face reconstruction등 얼굴 생성 모델에 대한 정량적인 성능 평가 방법(NME)과 정성적인 성능 평가 방법 (자연스러움, 사실성 등) 중 어느 것이 더 중요한가요?
	- 이런 평가방법은 굉장히 중요하지만, 이 평가방법은 이걸 어디다 쓸거냐에 따라서 다르게 생각을 해봐야하는거구요.
	- 자연스러움/정성적인 성능평가는 되게 어렵죠. 사람들이 직접해야하니까 다양하게 해볼 수 없는 문제가 있는데.
	- 이걸 가지고 뭘할까에 따라 다를 것 같고…
	- 기술 자체가 문제가 아니다라는 것처럼, 평가 자체가 문제가 아니고, 사용하는 사람의 문제라고 생각해요.
		- 평가 방법을 만드는 데에도 악용될 수 있다는 이해는 고려가 되어야할것같습니다.
- 딥페이크처럼 악용될 가능성이 높은 기술이라도 계속 발전해야 한다고 생각하시나요? 발전한다면 어떤 방향으로 해야 할까요?
	- 어려운 문제인것같아요. 개인적으로 저는 발전해야한다고 생각합니다.
	- 판도라의 상자를 연셈이 되죠. 그렇지만 어찌됐든 이 기술의 발전을 막을수는 없다고 보구요.
		- 이미 굉장히 많이 발전했고 앞으로 더 많이 발전을 할건데.
		- 당연히 어렵겠지만 더 좋게 만드는 방향으로 발전을 해야겠죠.
- AI의 발전으로 사라질 수 있는 직업들이 이해관계자와 또는 다양한 관점(윤리적, 경제적)에서 볼 때, 결론이 매우 다를 수 있다고 생각하는데요. 마스터님께서는 이런 논의관련해서 어떤 백그라운드의 배경으로 어떤 방향의 지향점을 생각하시는지 궁금합니다.
	- 저는 당연히 기술개발하는 관점에서 생각하구요.
	- 인류/사회에 대해 생각을 하면
		- 대체되고 있는 직종이 사실 지금도 사회적 약자들이기 때문인것같아요.
			- 단순노동자들.
		- 기술이 발전하면서 국가간/개인간 양극화가 더 발전될 확률이 높죠.
	- 개인의 관점에서 생각해보면,
		- 어떻게 내 일이 대체되지 않고 효율적으로 더 좋은 품질의 서비스를 해줄수 있을까로 가야하지만…
	- 기업들의 관점을 생각해보면
		- 사람을 대체해서 더 어떻게 잘할 수 있을까…
	- AI 기술을 개발하는 사람들/기업/정부가 다 같이 항상 논의를 해야하는 것 같아요.
		- 사회적 이슈, 빈부격차, 양극화, 최소화하면서 더 나아지는 방향으로 가면서 효율성을 높이는…
		- 너무 어려운 문제들이 얽혀있네요.
- 안녕하세요 마스터님. 강의를 통해 AI를 활용해서 기후변화를 예측하는 연구가 진행되고 있다고 말씀해주셨는요. 단순하게 생각하기로는 딥러닝보다 (딥러닝이 아닌)머신러닝이 아무래도 에너지를 덜 쓸 것 같습니다.이러한 이유로 기후변화를 예측하는 연구는 딥러닝을 제외한 머신러닝을 이용하는 방안으로 진행되는 경향이 있는지 궁금합니다. 또 이러한 연구에 대해서는 어떠한 키워드로 검색을 하면 자세히 알 수 있는지 여쭤보고 싶습니다
	- 딥러닝을 쓰는 이유는 더 정확한 예측을 하려고 쓰는거거든요.
		- 그래서 데이터가 더 커야하고 모델이 더 복잡해지고, 따라서 계산량이 늘어서, 에너지를 더 많이 쓰는.
	- 일단 명확하게 말씀드리자면 딥러닝과 머신러닝이 다르지는 않아요.
	- 딥러닝이 아닌 좀 더 light한 모델로 할수 있느냐 라는 것 같은데, 그렇게 간단한 해결책은 아닌것같아요.
		- 정확한 예측을 하기 위해서는 더 모델이 크고 데이터가 더 커야해요.
		- efficiency를 위해서는 어떻게 모델의 complexity를 그렇게 높이지는 않으면서 더 정확하게 할수있을까 -> 이건 연구가 계속되고있고
	- 기후변화를 위해서 딥러닝을 쓰느냐 안쓰느냐는 좀 사이드 이슈인것 같은데, 딥러닝을 포함한 방식들이 아직까지는 효과가 있던건 못봤던 것같아요.
		- 사실 지금은 아직은 통계/확률적인 모델링을 더 많이 하고 있는 것 같아요.
		- 기후연구가 ML연구로 아직 많이 안들어와서, 데이터 availability가 없는것도 있고…
- 딥러닝을 활용하는데 많은 전력량을 사용하는것이 앞으로 문제가 될것이라 생각합니다. 지속적으로 기존의 딥러닝 모델발전이 전력 문제를 해결할 수 있을것이라 생각하시나요? 아니면 전혀 다른 새로운 모델이 나와서 문제를 해결해 주실 거라 생각하나요? 교수님의 의견이 궁금합니다. 위의 문제를 해결책이 미래에 나타난다면 그 해결책은 새로운 하드웨어라고 생각하시나요 아니면 딥러닝 모델의 변화라고 생각하시나요?
	- 둘 다인 것 같구요.
	- 모델은 사실 efficiency를 위해 충분히 많이 연구가 되고 있는데,
	- 저는 제가 모르는 분야라 그런지 하드웨어나 컴퓨팅 시스템의 발전이 정말 중요하다고 생각해요.
		- 데이터가 많고, 모델이 한대임에도 불구하고 모델이 에너지를 덜 효율적으로 쓴다? 이건 하드웨어의 발전에 크게 좌우될거라고 생각해요.
		- TPU보다 좀 더 어떤 모델에 특화된 하드웨어들이 나올수도 있지 않을까요?
			- 그런 연구들이 조금 되고있고, 이런 경우 트레이드오프가 되겠죠.(범용성과 효율성)
	- 저는 모델 쪽의 변화도 있지만, 어찌됐든 하드웨어/시스템의 발전이 중요하다고 생각해요. 그쪽에서 큰 변화가 있을거라고 생각합니다.
- 자율주행 윤리 문제로 트롤리 딜레마(10명 vs 1명(나))를 들어본적이 있습니다. 쉽게 결정하기 어려운 문제를 AI가 어떻게 할것인지 우리모두 고민해보자 정도로 요약할 수 있을꺼 같습니다. 저희는 그 다음이 궁금합니다. 실제로 결정한 사례가 있나요? 그래서 테슬라는 어떻게 하는걸까요? 아니면 여전히 남겨진 숙제 인가요?
	- 제가 자율주행에 전문성이 없긴 해서…ㅠㅠㅎㅎ
	- 사실 테슬라를 사도 사고가 나면 회사에서 책임을 안지잖아요. 자율주행 모드가 있지만 그걸 쓰면 안된다…
		- 운전자 책임이기 때문에,
		- 이게 윤리적으로 어렵다기보다는 법적으로 어려워서 그런것같아요. 자동차 회사가 얼마나 큰 법적 책임을 져야하는지.
	- 트롤리 문제도 굉장히 이론적인 문제이기 때문에, 사실 현실에는 더 복잡한 문제들이 있잖아요.
		- 그런것들은 다 법적으로 해결이 되지 않았을 것 같아요.
		- 마스터로서 큰 도움이 되지 못한거같아 죄송하네요 ㅠㅠ

## 캠퍼님들께 마지막으로 하고싶은 말씀

굉장히 좋은 공부들을 하고 계신것같고, 다들 젊으신 분들 같은데 사실 우리 인생이 AI로 인한 trasition period에 있는것같아요.
기술을 배우는 것도 좋지만, 이걸 가지고 내가 어떻게 사회를 변화시킬수 있을까에 대해 항상 고민하시면 좋을것같아요.
한국에서도 AI가 많이 발전하고 있지만 발전 방향에 대해서만 생각하다보니까 경쟁에만 집중되고있어요.
그것보단 전체적으로 기술이 어떤 방향으로 가야할까에 대해서 고민하셨으면 좋겠습니다.

Select a repo