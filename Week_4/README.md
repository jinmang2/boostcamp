### [Week 4] 자연어처리 - 주재걸 교수님

[[Day 16] NaiveBayse and Word2Vec](https://github.com/ydy8989/boostcamp/tree/main/Week_4/Day_1)

1. Week 4 Overview
2. 데이터셋 준비하기
3. Introduction to NLP
4. Bag-of-Words
5. Word2Vec, GloVe
6. (실습 1강) Naive Bayes classifier 구현
7. (실습 2강) Word2Vec 구현
8. 과제 - NLP Preprocessing

- 대표적인 영어/한국어 전처리 기법들을 소개하고 알아봅니다. 

[[Day 17] Type of RNNs](https://github.com/ydy8989/boostcamp/tree/main/Week_4/Day_2)

1. Basics of Recurrent Neural Networks (RNNs)
2. LSTM, GRU
3. (실습 3강) Basic RNN 실습
4. (실습 4강) LSTM, GRU 실습
5. 과제 - Neural Machine Translation 모델을 만들기 위한 전처리

- 번역 모델을 만들기 이전 필요한 전처리 기법에 대해 학습합니다.

[[Day 18] Attention / Beam Search and BLEU](https://github.com/ydy8989/boostcamp/tree/main/Week_4/Day_3)

1. Sequence to sequence with attention
2. Beam search and BLEU score
3. (실습 5강) Seq2Seq 구현
4. (실습 6강) Seq2Seq with Attention 구현
5. 조교님의 과제 해설 강의
6. 과제 - Fairseq library를 이용한 sequence to sequence 모델 만들기

- sequence를 modeling할 수 있는 대표적인 library 중 하나인 fairseq을 이용해 직접 seq2seq 모델을 만들어 학습시켜봅니다.

[[Day 19] Transformer I, II](https://github.com/ydy8989/boostcamp/tree/main/Week_4/Day_4)

1. Transformer
2. (실습 7강) Multi-head Attention 구현
3. (실습 8강) Masked Multi-head Attention 구현
4. 조교님의 과제 해설 강의
5. 과제 - Byte Pair Encoding

- Out-of-Vocabulary 문제를 해결하기 위해 등장한 Byte Pair Encoding을 직접 구현해봅니다.

[[Day 20] Pre-training Models](https://github.com/ydy8989/boostcamp/tree/main/Week_4/Day_5)

1. Self-supervised Pre-training Models
2. Other Self-supervised Pre-training Models
3. (실습 9강) HuggingFace's Transformers 1
4. (실습 10강) HuggingFace's Transformers 2
5. 조교님의 과제 해설 강의
6. 과제 - Named Entity Recognition(NER) with Transformers library
7. 마스터클래스
