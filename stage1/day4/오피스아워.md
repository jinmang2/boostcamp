

- 학습 과정과 빨리 시킬수 있는 방법을 공유해준다.
- others는 이외의 내용들을 말함



- 



- 레이블을 스무딩하면 모델이 더 잘 이해한다. 
- 1과 0이 아니라...
- 레이블을 스무딩 시켜라!



1.2. knowledge distillation

- augmentation하면 랜덤 크랍된다.
- 이때의 문제점?...
	- 고양이가 업는 부분을 크랍하면 망함





티쳐모델을 학습하고, 스튜던트 모델을 학습하여서....반복...

티쳐 모델이 좀더 크고, 스튜던트가 더 작음





개와 고양이가 있으면, 조금 섞어서 원핫 벡터를 레이블 linear interpolation하는 거임



코사인 러닝레이트 디케이....

학습률을 학습 epoch에 따라서 조정하는거임

논문에서는 코사인 디케이를 많이 쓰긴함...



이미지 부르고, 순전파, 로스계산, 역전파하고 업데이트 반복...

베이스라인 전처리 과정을 좀 참고하라...



- 디스틸레이션이랑 레이블 스무딩은 같이쓰면 안좋다는 결과가 있다.



- resolution이 train과 test에서 다르기 때문에 안좋게 나온다.



dit는 vit랑 구조는 비슷한데, 약간의 테크닉을 사용한...



224 by 224로 학습을 시킨다. 

작은걸로 하면 학습을 빨리 시킬 수 있다. 

학습 시키고 2stage에서 384, 384로 학습시킨다.

뒷단 레이어만 fine tuning시킨다. 

random augmentation, auto augmentation





배치사이즈가 클수록 학습은 빠르지만 수렴이 어려움...

러닝레이트를 높이면 좋은데 0.1 x batch/256로하면 좋음..

웜업 : 서서히 높이자! - 코사인 디케이

첨부터 너무 크면 파라미터가 튈수가 있어서...서서히 올리다가 안정화되면 그때부터!



감마 : 스케일링

베타 : 트랜지션 팩터



비트 타입을 바꾸면 조금 성능 개선을 유도할 수 있다. 

16비트 파이토치에서도 가능하니깐 확인해봐라 





